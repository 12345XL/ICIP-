"""
# æ–‡ä»¶è¯´æ˜ï¼ˆclip/build_visual_bank.pyï¼‰

- **æ–‡ä»¶ä½œç”¨**ï¼šä½¿ç”¨ CLIP å°†è®­ç»ƒé›†æ­£å¸¸å›¾ç‰‡ç¼–ç ä¸ºå…¨å±€ç‰¹å¾ä¸ Patch ç‰¹å¾ï¼Œå¹¶æ„å»ºåˆ†å±‚ FAISS å‘é‡åº“ã€‚
- **è¿è¡Œæ–¹å¼**ï¼šåœ¨é¡¹ç›®æ ¹ç›®å½•æ‰§è¡Œ `python Scripts/clip/build_visual_bank.py`ï¼Œä¼šè‡ªåŠ¨éå† `DATASET_ROOT` ä¸‹æ‰€æœ‰ç±»åˆ«ã€‚
- **è¾“å‡ºç»“æœ**ï¼šåœ¨å½“å‰å·¥ä½œç›®å½•ç”Ÿæˆ `embeddings/` ç›®å½•ï¼ŒåŒ…å« `<category>_global.index`ã€`<category>_patch.index` å’Œ `*_meta.pkl`ã€‚
- **åˆ†ç±»è§’è‰²**ï¼šå½’å±äº `clip` åˆ†ç±»ï¼Œæ˜¯ CLIP è§†è§‰æ£€ç´¢ç®¡çº¿ä¸­çš„ã€Œç´¢å¼•æ„å»ºã€è„šæœ¬ã€‚
"""

import os
import clip
import torch
import faiss
import pickle
import numpy as np
from PIL import Image
from tqdm import tqdm

# --- é…ç½®å‚æ•° ---
DATASET_ROOT = "/data/XL/å¤šæ¨¡æ€RAG/DataSet/MVTec-AD"
OUTPUT_DIR = "./embeddings"
MODEL_NAME = "ViT-L/14@336px"
STRIDE_RATIO = 0.5
BATCH_SIZE = 64

class HierarchicalBankBuilder:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸš€ Loading CLIP ({MODEL_NAME}) on {self.device}...")
        self.model, self.preprocess = clip.load(MODEL_NAME, device=self.device)
        self.input_size = self.model.visual.input_resolution
        self.stride = int(self.input_size * STRIDE_RATIO)
        os.makedirs(OUTPUT_DIR, exist_ok=True)

    def process_category(self, category):
        """å¤„ç†å•ä¸ªç±»åˆ«çš„æ ¸å¿ƒé€»è¾‘"""
        print(f"\nâš¡ Processing Category: {category}")
        train_good_path = os.path.join(DATASET_ROOT, category, "train", "good")
        
        if not os.path.exists(train_good_path):
            return

        # å®¹å™¨
        global_features = []   # å­˜æ•´å›¾ç‰¹å¾ (ç”¨äºé˜¶æ®µ1æ£€ç´¢)
        patch_features = []    # å­˜Patchç‰¹å¾ (ç”¨äºé˜¶æ®µ2å¯¹é½)
        
        # ç´¢å¼•æ˜ å°„
        global_metadata = []   # {img_path}
        patch_metadata = []    # {parent_img_idx, coords} -> parent_img_idx æŒ‡å‘ global_metadata

        img_files = [f for f in os.listdir(train_good_path) if f.lower().endswith(('.png', '.jpg'))]
        
        for img_idx, img_file in enumerate(tqdm(img_files, desc=f"Encoding {category}")):
            img_path = os.path.join(train_good_path, img_file)
            
            try:
                # --- 1. è¯»å–ä¸å…¨å±€ç‰¹å¾ ---
                pil_img = Image.open(img_path).convert("RGB")
                global_input = self.preprocess(pil_img).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    g_feat = self.model.encode_image(global_input)
                    g_feat /= g_feat.norm(dim=-1, keepdim=True)
                
                global_features.append(g_feat.cpu().numpy())
                global_metadata.append(img_path)
                
                # --- 2. æå– Patches ---
                w, h = pil_img.size
                batch_patches = []
                batch_coords = []
                
                for y in range(0, h - self.input_size + 1, self.stride):
                    for x in range(0, w - self.input_size + 1, self.stride):
                        box = (x, y, x + self.input_size, y + self.input_size)
                        patch = pil_img.crop(box)
                        batch_patches.append(self.preprocess(patch))
                        batch_coords.append((x, y))
                
                if batch_patches:
                    patch_tensor = torch.stack(batch_patches).to(self.device)
                    with torch.no_grad():
                        p_feat = self.model.encode_image(patch_tensor)
                        p_feat /= p_feat.norm(dim=-1, keepdim=True)
                    
                    patch_features.append(p_feat.cpu().numpy())
                    
                    # è®°å½• Patch çš„â€œçˆ¶çº§â€æ˜¯è°ï¼Œä»¥åŠåæ ‡
                    for (x, y) in batch_coords:
                        patch_metadata.append({
                            "parent_idx": img_idx, # å…³è”åˆ° global_metadata[img_idx]
                            "coords": (x, y)
                        })
                        
            except Exception as e:
                print(f"Error: {e}")

        # --- 3. å­˜ç›˜ (åˆ†å±‚å­˜å‚¨) ---
        if global_features:
            # A. å…¨å±€ç´¢å¼•
            g_emb = np.concatenate(global_features, axis=0).astype('float32')
            g_index = faiss.IndexFlatIP(g_emb.shape[1])
            g_index.add(g_emb)
            faiss.write_index(g_index, os.path.join(OUTPUT_DIR, f"{category}_global.index"))
            
            # B. å±€éƒ¨ Patch ç´¢å¼•
            p_emb = np.concatenate(patch_features, axis=0).astype('float32')
            p_index = faiss.IndexFlatIP(p_emb.shape[1])
            p_index.add(p_emb)
            faiss.write_index(p_index, os.path.join(OUTPUT_DIR, f"{category}_patch.index"))
            
            # C. å…ƒæ•°æ®
            with open(os.path.join(OUTPUT_DIR, f"{category}_meta.pkl"), 'wb') as f:
                pickle.dump({
                    "global_paths": global_metadata,
                    "patch_info": patch_metadata
                }, f)
            
            print(f"âœ… {category}: {g_index.ntotal} images, {p_index.ntotal} patches.")

if __name__ == "__main__":
    builder = HierarchicalBankBuilder()
    categories = sorted([d for d in os.listdir(DATASET_ROOT) if os.path.isdir(os.path.join(DATASET_ROOT, d))])
    for cat in categories:
        builder.process_category(cat)
