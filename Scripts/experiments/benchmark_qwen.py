"""
# æ–‡ä»¶è¯´æ˜ï¼ˆexperiments/benchmark_qwen.pyï¼‰

- **æ–‡ä»¶ä½œç”¨**ï¼šå¯¹æ¯”ã€Œç›´æ¥é—®å¤§æ¨¡å‹ã€å’Œã€ŒDINO RAG + çƒ­åŠ›å›¾ + è¡Œä¸šçŸ¥è¯†ã€ä¸¤ç§æ¨¡å¼ä¸‹çš„ç¼ºé™·æ£€æµ‹æ•ˆæœã€‚
- **è¿è¡Œæ–¹å¼**ï¼šåœ¨é¡¹ç›®æ ¹ç›®å½•æ‰§è¡Œ `python Scripts/experiments/benchmark_qwen.py`ï¼Œç¡®ä¿å·²ç”Ÿæˆ `results_dino_final` å’Œ `knowledge_corpus.json`ã€‚
- **è¾“å‡ºç»“æœ**ï¼šç”Ÿæˆ `benchmark_results_<CATEGORY>.csv`ï¼Œå¹¶åœ¨ç»ˆç«¯æ‰“å° Accuracy / Precision / Recall / F1 å¯¹æ¯”æŠ¥å‘Šã€‚
- **åˆ†ç±»è§’è‰²**ï¼šå½’å±äº `experiments` åˆ†ç±»ï¼Œæ˜¯æ•´æ¡å¤šæ¨¡æ€ RAG æµæ°´çº¿çš„ç»¼åˆè¯„æµ‹è„šæœ¬ã€‚
"""

import os
import sys
import json
import argparse
import torch
import cv2
import pandas as pd
from PIL import Image
from tqdm import tqdm
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

SCRIPTS_DIR = os.path.dirname(os.path.dirname(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPTS_DIR)
sys.path.append(SCRIPTS_DIR)
sys.path.append(PROJECT_ROOT)
from config import (
    DATASET_ROOT,
    QWEN_MODEL_PATH,
    KNOWLEDGE_PATH,
)

MODEL_PATH = QWEN_MODEL_PATH

class IndustrialBenchmark:
    def __init__(self, category, panel_dir, out_dir):
        """åˆå§‹åŒ–åŸºå‡†è¯„æµ‹å™¨ï¼šåŠ è½½ Qwen3-VL æ¨¡å‹ä¸å¤„ç†å™¨ï¼Œå¹¶å‡†å¤‡çŸ¥è¯†åº“ã€‚
        å°ç™½è§£é‡Šï¼šè¿™ä¸€æ­¥å°±æ˜¯æŠŠå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆèƒ½çœ‹å›¾ä¼šè¯´è¯ï¼‰ä»æœ¬åœ°è·¯å¾„åŠ è½½è¿›æ¥ï¼Œ
        åŒæ—¶åŠ è½½ä¸€ä¸ªä¸“é—¨çš„å¤„ç†å™¨ï¼ˆProcessorï¼‰ç”¨æ¥æŠŠå›¾ç‰‡å’Œæ–‡å­—å˜æˆæ¨¡å‹èƒ½ç†è§£çš„æ ¼å¼ã€‚
        å¦å¤–è¿˜æŠŠä½ å†™å¥½çš„è¡Œä¸šçŸ¥è¯†åº“è¯»è¿›æ¥ï¼Œåé¢ä½œä¸ºæç¤ºè¯çš„ä¸€éƒ¨åˆ†ä½¿ç”¨ã€‚"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.category = category
        self.panel_dir = panel_dir
        self.out_dir = out_dir
        print(f"ğŸš€ Loading Qwen Model from: {MODEL_PATH}")
        
        # åŠ è½½æ¨¡å‹ (è‡ªåŠ¨é€‚é…æ˜¾å­˜)
        self.model = Qwen3VLForConditionalGeneration.from_pretrained(
            MODEL_PATH,
            dtype="auto",
            device_map="auto",
        )
        self.processor = AutoProcessor.from_pretrained(MODEL_PATH)
        
        # åŠ è½½çŸ¥è¯†åº“ (ç”¨äº RAG æ¨¡å¼)
        self.knowledge_text = self._load_knowledge()
        print("âœ… Model & Knowledge Loaded.")

    def _binary_from_text(self, text):
        s = (text or "").lower()
        head = s[:20]
        if ("yes" in head) or ("fail" in head) or ("defect" in head):
            return 1
        return 0

    def _load_knowledge(self):
        """è¯»å–å¹¶æ•´ç†å½“å‰ç±»åˆ«çš„è¡Œä¸šçŸ¥è¯†ï¼Œç”Ÿæˆå¯æ’å…¥æç¤ºè¯çš„æ–‡æœ¬ã€‚
        å°ç™½è§£é‡Šï¼šæŠŠ JSON é‡Œçš„çŸ¥è¯†ç­›é€‰å‡ºæ¥ï¼Œæ‹¼æˆä¸€æ®µè¯´æ˜æ–‡å­—ï¼Œ
        æ¨¡å‹çœ‹åˆ°è¿™æ®µè¯´æ˜å°±æ›´æ‡‚è¡Œä¸šæ ‡å‡†å’Œç¼ºé™·å®šä¹‰ã€‚"""
        with open(KNOWLEDGE_PATH, 'r', encoding='utf-8') as f:
            kb = json.load(f)
        
        # ç­›é€‰å½“å‰ç±»åˆ«çš„çŸ¥è¯†
        normal_desc = ""
        defects = []
        for item in kb:
            if item['category'] == self.category:
                if item['type'] == 'normal_criteria':
                    normal_desc += item['content']
                else:
                    defects.append(f"- {item['key']}: {item['content']}")
        
        return f"**Normal Standards:**\n{normal_desc}\n\n**Potential Defects:**\n" + "\n".join(defects)

    def predict(self, image_path, prompt_text):
        """é€šç”¨æ¨ç†ï¼šç»™æ¨¡å‹ä¸€å¼ å›¾å’Œä¸€æ®µæ–‡å­—ï¼Œç”Ÿæˆå›ç­”æ–‡æœ¬ã€‚
        å°ç™½è§£é‡Šï¼šè¿™é‡Œå…ˆæŠŠä½ çš„é—®é¢˜å’Œå›¾ç‰‡æ‰“åŒ…æˆå¯¹è¯æ ¼å¼ï¼Œ
        ç„¶åç”¨å¤„ç†å™¨æŠŠå®ƒä»¬å˜æˆå¼ é‡ï¼Œæœ€åè®©æ¨¡å‹ç”Ÿæˆç­”æ¡ˆå¹¶è§£ç æˆå­—ç¬¦ä¸²ã€‚"""
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image_path},
                    {"type": "text", "text": prompt_text},
                ],
            }
        ]
        
        # é¢„å¤„ç†
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        ).to(self.device)

        # ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = self.model.generate(**inputs, max_new_tokens=128)
        
        # è§£ç 
        generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = self.processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]
        
        return output_text

    def run_experiment(self):
        """æ‰¹é‡è¿è¡Œè¯„æµ‹ï¼šéå†æµ‹è¯•é›†ï¼Œå¯¹æ—  RAG ä¸æœ‰ RAG ä¸¤ç§è®¾ç½®åˆ†åˆ«æ¨ç†å¹¶è®°å½•ç»“æœã€‚
        å°ç™½è§£é‡Šï¼šå¾ªç¯è·‘æµ‹è¯•é›†çš„å›¾ç‰‡ï¼Œå…ˆç›´æ¥é—®æ¨¡å‹ï¼ˆæ— çŸ¥è¯†ï¼‰ï¼Œ
        å†ç”¨æˆ‘ä»¬ç”Ÿæˆçš„æ‹¼å›¾+çŸ¥è¯†ï¼ˆæœ‰ RAGï¼‰å»é—®ä¸€æ¬¡ï¼Œå¯¹æ¯”ç»“æœå¹¶å­˜è¡¨æ ¼ã€‚"""
        test_root = os.path.join(DATASET_ROOT, self.category, "test")
        results = []
        
        # éå†æ‰€æœ‰æµ‹è¯•å­æ–‡ä»¶å¤¹
        subdirs = [d for d in os.listdir(test_root) if os.path.isdir(os.path.join(test_root, d))]
        
        print(f"ğŸ”¥ Starting Benchmark on {self.category}...")
        
        for dtype in subdirs:
            # æ ‡è®° Ground Truth (good=0, ç¼ºé™·=1)
            label = 0 if dtype == "good" else 1
            img_dir = os.path.join(test_root, dtype)
            
            # ä¸ºäº†å¿«é€Ÿå‡ºç»“æœï¼Œæ¯ä¸ªç±»å‹åªè·‘ 10 å¼ å›¾ (ä½ å¯ä»¥æ”¹æˆæ‰€æœ‰)
            files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.png', '.jpg'))][:10]
            
            for f in tqdm(files, desc=f"Testing {dtype}"):
                raw_img_path = os.path.join(img_dir, f)
                
                # --- Setting A: No RAG (Baseline) ---
                # åªæœ‰åŸå›¾ï¼Œæ²¡æœ‰èƒŒæ™¯çŸ¥è¯†ï¼Œç›´æ¥é—®
                prompt_a = "Look at this product image. Is there any defect? Answer strictly with 'Yes' or 'No' first, then explain."
                pred_a_text = self.predict(raw_img_path, prompt_a)
                pred_a_score = self._binary_from_text(pred_a_text)
                
                # --- Setting B: With RAG (Ours) ---
                # ä½¿ç”¨ inference_dino.py ç”Ÿæˆçš„ç»„åˆå›¾ (Visual Prompt)
                # ç»„åˆå›¾è·¯å¾„ (éœ€ç¡®ä¿ä½ ä¹‹å‰è·‘è¿‡ inference_dino.py)
                stem = f"{dtype}_{os.path.splitext(f)[0]}"
                rag_img_path = os.path.join(self.panel_dir, self.category, f"{stem}.png")
                
                if os.path.exists(rag_img_path):
                    prompt_b = f"""
ROLE: You are a Quality Assurance AI analyzing an industrial product.

INPUT LAYOUT:
- Left Panel: Original Product Image.
- Middle Panel: Golden Reference (Perfect Standard).
- Right Panel: Anomaly Score Map (heatmap).

HEATMAP DESCRIPTION:
1. The Right Panel is an anomaly score map produced by an algorithm.
2. Warmer colors (yellow/red) indicate higher probability of defect, cooler colors (blue) indicate lower probability.
3. The heatmap may contain noise or imperfect signals. Do not blindly trust every red pixel.
4. You must jointly consider the original image, the reference image and the heatmap before making a decision.

Domain Knowledge for {self.category}:
{self.knowledge_text}

TASK:
1. Decide whether this product is defective (answer Yes or No).
2. Provide a confidence score between 0 and 1 (0 = definitely normal, 1 = definitely defective).
3. Briefly describe the main suspicious region if any (location and type).

RESPONSE FORMAT (ENGLISH):
Line 1: "Answer: Yes" or "Answer: No"
Line 2: "Confidence: <number between 0 and 1>"
Line 3: One short sentence describing the key evidence.
"""
                    pred_b_text = self.predict(rag_img_path, prompt_b)
                    pred_b_score = self._binary_from_text(pred_b_text)
                else:
                    # å¦‚æœæ²¡æœ‰å¯¹åº”çš„ RAG å›¾ç‰‡ï¼ˆä¹‹å‰æ²¡ç”Ÿæˆï¼‰ï¼Œåˆ™è·³è¿‡æˆ–è®¾ä¸º -1
                    pred_b_score = -1 
                    pred_b_text = "RAG Image Missing"

                # è®°å½•ç»“æœ
                results.append({
                    "filename": f"{dtype}/{f}",
                    "gt_label": label,
                    "pred_no_rag": pred_a_score,
                    "pred_with_rag": pred_b_score,
                    "expl_no_rag": pred_a_text,
                    "expl_with_rag": pred_b_text
                })

        # ä¿å­˜è¡¨æ ¼
            df = pd.DataFrame(results)
            os.makedirs(self.out_dir, exist_ok=True)
            pred_csv = os.path.join(self.out_dir, "predictions.csv")
            df.to_csv(pred_csv, index=False)
            print(f"\nğŸ’¾ Raw results saved to {pred_csv}")
            return df

    def calculate_metrics(self, df):
        """è®¡ç®—ç¡¬æ€§æŒ‡æ ‡ï¼šAccuracyã€Precisionã€Recallã€F1ã€‚
        å°ç™½è§£é‡Šï¼šæŠŠé¢„æµ‹ç»“æœå’ŒçœŸå€¼åšå¯¹æ¯”ï¼Œç®—å‡ºå¸¸è§çš„åˆ†ç±»æŒ‡æ ‡ï¼Œ
        å¹¶æ‰“å°ä¸€ä¸ªå¯¹æ¯”æŠ¥å‘Šï¼Œæ–¹ä¾¿ä½ çœ‹ RAG æ˜¯å¦æå‡æ•ˆæœã€‚"""
        # è¿‡æ»¤æ‰ RAG å›¾ç‰‡ç¼ºå¤±çš„æ•°æ®
        df_valid = df[df["pred_with_rag"] != -1]
        
        y_true = df_valid["gt_label"].values
        y_pred_a = df_valid["pred_no_rag"].values
        y_pred_b = df_valid["pred_with_rag"].values
        
        def get_metrics(y_t, y_p):
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            return {
                "Accuracy": accuracy_score(y_t, y_p),
                "Precision": precision_score(y_t, y_p, zero_division=0),
                "Recall": recall_score(y_t, y_p, zero_division=0),
                "F1": f1_score(y_t, y_p, zero_division=0)
            }
            
        metrics_a = get_metrics(y_true, y_pred_a)
        metrics_b = get_metrics(y_true, y_pred_b)
        
        print("\n" + "="*40)
        print("ğŸ† Final Benchmark Report")
        print("="*40)
        print(f"{'Metric':<12} | {'No RAG (Baseline)':<18} | {'With RAG (Ours)':<18}")
        print("-" * 52)
        for k in metrics_a.keys():
            print(f"{k:<12} | {metrics_a[k]:.4f}{' '*12} | {metrics_b[k]:.4f}")
        print("="*40)
        metrics = {
            "Accuracy_no_rag": metrics_a["Accuracy"],
            "Precision_no_rag": metrics_a["Precision"],
            "Recall_no_rag": metrics_a["Recall"],
            "F1_no_rag": metrics_a["F1"],
            "Accuracy_with_rag": metrics_b["Accuracy"],
            "Precision_with_rag": metrics_b["Precision"],
            "Recall_with_rag": metrics_b["Recall"],
            "F1_with_rag": metrics_b["F1"],
        }
        os.makedirs(self.out_dir, exist_ok=True)
        metrics_path = os.path.join(self.out_dir, "metrics.json")
        with open(metrics_path, "w", encoding="utf-8") as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Metrics saved to {metrics_path}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--run_dir",
        type=str,
        required=True,
        help="Path to a run directory under Results/exp/",
    )
    args = parser.parse_args()
    run_dir = args.run_dir
    cfg_path = os.path.join(run_dir, "run_config.json")
    if not os.path.exists(cfg_path):
        raise FileNotFoundError(f"run_config.json not found in {run_dir}")
    with open(cfg_path, "r", encoding="utf-8") as f:
        cfg = json.load(f)
    category = cfg.get("category")
    if not category:
        raise ValueError("category is missing in run_config.json")
    panel_dir = os.path.join(run_dir, "inference", "panels")
    out_dir = os.path.join(run_dir, "benchmark_qwen")
    benchmark = IndustrialBenchmark(category=category, panel_dir=panel_dir, out_dir=out_dir)
    df_results = benchmark.run_experiment()
    benchmark.calculate_metrics(df_results)


if __name__ == "__main__":
    main()
